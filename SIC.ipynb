{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PiXh1xeoTm4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9b8de9-3815-4ec7-8453-7951a4e33be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ_tsDThZHIE",
        "outputId": "e773ebea-be98-4fa4-cce5-8b6dfd1f2031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFN2yRKDXws",
        "outputId": "c98a0104-0c34-48da-85c5-f173e706dc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#import thư viện"
      ],
      "metadata": {
        "id": "ZbnwSmFla7Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR# mô hình chạy thử\n",
        "from sklearn.svm import SVC# mô hình chạy thử\n",
        "from sklearn.neighbors import KNeighborsClassifier# mô hình chạy thử\n",
        "import pandas as pd# lấy dữ liệu\n",
        "import numpy as np # tính toán\n",
        "import matplotlib.pyplot as plt # vẽ đồ thị\n",
        "from sklearn.model_selection import train_test_split # Chia dữ liệu\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix #đánh giá mô hình\n",
        "import re # tiền xử lý\n",
        "import string # tiền xử lý\n",
        "from sklearn.decomposition import PCA # giảm chiều dữ liệu\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Tính tfidf\n",
        "import joblib # lưu model\n",
        "from joblib import load as ld\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "6sjZr1aqa7GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hàm"
      ],
      "metadata": {
        "id": "-chsJnlkaow9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vietnamese_stop_words = [\n",
        "              'là', 'và', 'có', 'trên', 'cho', 'một', 'các', 'được', 'như', 'với',\n",
        "              'của', 'ở', 'khi', 'đã', 'còn', 'thì', 'này', 'bởi', 'đó', 'để', 'năm',\n",
        "              'ngày', 'vào', 'sau', 'tại', 'trong', 'rằng', 'đến', 'từ'\n",
        "              ]"
      ],
      "metadata": {
        "id": "jVOuwTx6a2eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('/content/drive/MyDrive/SIC/model_doc2vec')"
      ],
      "metadata": {
        "id": "JP3sCzu4a3dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm làm sạch dữ liệu <nên dùng trước khi vector hoá dữ liệu>\n",
        "def convertData(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]','',text)\n",
        "  text = re.sub(\"\\\\W\",\" \",text)\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
        "  text = re.sub('<.*?>+',b'',text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
        "  text = re.sub('\\w*\\d\\w*','',text)\n",
        "  text = re.sub(' +', ' ',text)\n",
        "  text = text.strip()\n",
        "  text = ' '.join([word for word in text.split() if word not in vietnamese_stop_words])\n",
        "  return text"
      ],
      "metadata": {
        "id": "ZqLPFH-4azWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Cosine_similarity"
      ],
      "metadata": {
        "id": "2IydxwjDsmK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm để tính Cosine\n",
        "def cosine_similarity(x_vec,y_vec, flag):\n",
        "  cosine_similarities = x_vec.dot(y_vec.T)\n",
        "  if flag == True:# Trường hợp trả kết quả với dữ liệu tfIdf\n",
        "    return cosine_similarities.toarray().squeeze()#\n",
        "  else:# Trường hợp trả kết quả với dữ liệu ebd\n",
        "    ll = []\n",
        "    for i in range(cosine_similarities.shape[0]):\n",
        "      ll.append(cosine_similarities[i].max())\n",
        "    return ll"
      ],
      "metadata": {
        "id": "Y2Q2w-DAar8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm kiểm thử cho đánh giá độ tương đồng Cosine\n",
        "def Cosine_sml(fake_news,real_vectors):\n",
        "  # lấy chỉ số của từng fake_news\n",
        "  content = [convertData(txt) for txt in fake_news['Fake_Content']]# làm sạch từng bài\n",
        "  # vector hoá từng bài\n",
        "  vectors_content = vectorizer.transform(content)\n",
        "  cosine_similarities = cosine_similarity(vectors_content,real_vectors, True) # gọi hàm để tính Cosine\n",
        "  danhgia_f = 0\n",
        "  for idx, x in enumerate(cosine_similarities):\n",
        "    if x.max() < 0.5:\n",
        "      danhgia_f +=1\n",
        "  print(f'Tổng số tin giả của fake_news : {len(fake_news)}')\n",
        "  print(f'Số lượng tin được phát hiện có khả năng là giả của fake_news: {danhgia_f}')\n",
        "  print(f\"Độ chính xác khi dự đoán tin giả : {danhgia_f/len(fake_news)}\")\n",
        "  print('_____________________________________________________________________________________')\n",
        "  return danhgia_f/len(fake_news)"
      ],
      "metadata": {
        "id": "x3E0L7_Fpvjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm kiểm thử cho đánh giá độ tương đồng Cosine\n",
        "def Cosine_sml_ebd(fake_vectors,real_vectors):#fake_vectors va real_vectors\n",
        "  cosine_similarities = cosine_similarity(fake_vectors,real_vectors, False)\n",
        "  danhgia_f = 0\n",
        "  lt = list()\n",
        "  for idx, x in enumerate(cosine_similarities):\n",
        "    lt.append(x.max())\n",
        "  scalling = StandardScaler()\n",
        "  lt = scalling.fit_transform(np.array(lt).reshape(-1,1))\n",
        "  for idx, x in enumerate(lt):\n",
        "    if x.max() < 0.5:\n",
        "      danhgia_f +=1\n",
        "  print(f'Tổng số tin giả của fake_ : {len(fake_vectors)}')\n",
        "  print(f'Số lượng tin được phát hiện có khả năng là giả của fake_: {danhgia_f}')\n",
        "  print(f\"Độ chính xác khi dự đoán tin giả : {danhgia_f/len(fake_vectors)}\")\n",
        "  print('_____________________________________________________________________________________')\n",
        "  return danhgia_f/len(fake_vectors)"
      ],
      "metadata": {
        "id": "xz6co93Gnko_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gọi dữ liệu test"
      ],
      "metadata": {
        "id": "wbI66v_neSve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#real_news là phần tập dữ liệu gốc (8943 bài viết-tin thật) chưa convert và chưa vector hoá\n",
        "# 2 tập này chưa xử lý gì hết\n",
        "real_news = pd.read_csv('/content/drive/MyDrive/Data_/Data_test_300/DATA_.csv')#Tập tin có thể là thật\n",
        "teptingia = pd.read_csv('/content/drive/MyDrive/Data_/dulieutin_gia_4000.csv')#Tập tin có thể là giả\n",
        "#2 tập ebd này đã được dùng bert để vector hoá\n",
        "ebd_teptingia = pd.read_csv('/content/drive/MyDrive/Data_/Fake_Ebd_4000.csv') # 4000 dữ liệu đầu tiên\n",
        "ebd_teptinthat = pd.read_csv('/content/drive/MyDrive/SIC/Embeddings_DATA.csv')\n",
        "real_news.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "# real_news.columns = ['Content']\n",
        "\n",
        "real_news = real_news[4000:]\n",
        "real_news.reset_index(inplace=True)\n",
        "real_news.drop(columns=['index'],inplace=True)\n",
        "teptingia.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "teptingia.reset_index(inplace=True)\n",
        "teptingia.drop(columns=['Unnamed: 0.1'],inplace=True)\n",
        "teptingia.drop(columns=['index'],inplace=True)\n",
        "ebd_teptinthat.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "ebd_teptingia.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "ebd_teptinthat = ebd_teptinthat[4000:]\n",
        "ebd_teptinthat.reset_index(inplace=True)\n",
        "ebd_teptinthat.drop(columns=['index'],inplace=True)"
      ],
      "metadata": {
        "id": "lZrNj4xseSH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201de636-840a-4860-819b-74c0805619e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-fd2884e2c613>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  real_news.drop(columns=['index'],inplace=True)\n",
            "<ipython-input-12-fd2884e2c613>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ebd_teptinthat.drop(columns=['index'],inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ketqua_tfidf = list()\n",
        "ketqua_ebd = list()"
      ],
      "metadata": {
        "id": "BGyanmEjdhSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acr = list()\n",
        "prec = list()\n",
        "f1 = list()"
      ],
      "metadata": {
        "id": "7KVXI4GYnQRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Độ tương đồng Cosine"
      ],
      "metadata": {
        "id": "ICP1z9Hrwr2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>TfIDF</h1>"
      ],
      "metadata": {
        "id": "2sIz-4WgxwPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "real_vectors = vectorizer.fit_transform(real_news['Content'])"
      ],
      "metadata": {
        "id": "LxQEArq6UTWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ketqua_tfidf.append(Cosine_sml(teptingia,real_vectors))#tfidf"
      ],
      "metadata": {
        "id": "yGTYknZbkKUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e596bab7-ae38-425d-a34f-8190ecc8072e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng số tin giả của fake_news : 4000\n",
            "Số lượng tin được phát hiện có khả năng là giả của fake_news: 2170\n",
            "Độ chính xác khi dự đoán tin giả : 0.5425\n",
            "_____________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Bert</h1>"
      ],
      "metadata": {
        "id": "1xUkK3N3xp2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ketqua_ebd.append(Cosine_sml_ebd(ebd_teptingia,ebd_teptinthat))"
      ],
      "metadata": {
        "id": "U6WaA48p5HmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c456a2f0-736c-41ee-cb37-8340261045d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng số tin giả của fake_ : 4000\n",
            "Số lượng tin được phát hiện có khả năng là giả của fake_: 2828\n",
            "Độ chính xác khi dự đoán tin giả : 0.707\n",
            "_____________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVC - KNN"
      ],
      "metadata": {
        "id": "Zz75VbNjgtva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sử dụng tfIdf để vector hoá"
      ],
      "metadata": {
        "id": "EumKuvJtfB4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM_SVC"
      ],
      "metadata": {
        "id": "Q3sRb5AuVWV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gọi model\n",
        "svc_model = joblib.load('/content/drive/MyDrive/SIC/svc_model_update.pkl')"
      ],
      "metadata": {
        "id": "qIF2QPMl63Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Làm sạch\n",
        "X_Fake_tfidf = [convertData(teptingia['Fake_Content'][i]) for i in range(len(teptingia['Fake_Content']))]\n",
        "X_Real_tfidf = [convertData(real_news['Content'][i]) for i in range(len(real_news['Content']))]\n",
        "# Tổng hợp và đánh nhãn\n",
        "X = X_Real_tfidf + X_Fake_tfidf\n",
        "Y = [1]*len(X_Real_tfidf) + [0]*len(X_Fake_tfidf)\n",
        "#Chia dữ liêu train và test : 80 20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XbmfoEuJl_ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = joblib.load('/content/drive/MyDrive/SIC/vectorizer_tfidf.joblib')\n",
        "X_train_tfidf = vectorizer.transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "irZnXa6anQst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "ketqua_tfidf.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "KgKit75YoBdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41220362-aee8-4e08-c9b6-fa7a4be6f91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 78.42%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.79      0.77       819\n",
            "           1       0.82      0.78      0.80       970\n",
            "\n",
            "    accuracy                           0.78      1789\n",
            "   macro avg       0.78      0.79      0.78      1789\n",
            "weighted avg       0.79      0.78      0.78      1789\n",
            "\n",
            "Confusion Matrix:\n",
            "[[651 168]\n",
            " [218 752]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "xPok29YuuZsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = ld('/content/drive/MyDrive/SIC/knn_model_Update.joblib')"
      ],
      "metadata": {
        "id": "2N_OAXfvua7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "ketqua_tfidf.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "3FsTgfAcupTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "01edbdf9-3c25-42fa-82d6-1ca26286b1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-aef9452bb5e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mketqua_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {:.2f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# In that case, we do not need the distances to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# the weighting so we do not compute them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mneigh_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_pairwise_distances_reductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             results = ArgKmin.compute(\n\u001b[0m\u001b[1;32m    823\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \"\"\"\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             return ArgKmin64.compute(\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\u001b[0m in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_original_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sử dụng bert để vector hoá"
      ],
      "metadata": {
        "id": "y20GStMGfJMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model_ebd = joblib.load('/content/drive/MyDrive/SIC/svc_model_ebd.pkl')"
      ],
      "metadata": {
        "id": "zqbWaSp6DiXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bert = pd.concat([ebd_teptinthat,ebd_teptingia])\n",
        "y_bert = [1]*len(ebd_teptinthat) + [0]*len(ebd_teptingia)\n",
        "X_bert.reset_index(inplace=True)\n",
        "X_bert.drop(columns=['index'],inplace=True)"
      ],
      "metadata": {
        "id": "Yaka-PwzO4vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalling = StandardScaler()\n",
        "X_bert = scalling.fit_transform(X_bert)"
      ],
      "metadata": {
        "id": "pKg1DurY5LDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_bert, y_bert, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3rd0_dJ42r1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc_model_ebd.predict(X_test_b)\n",
        "accuracy = accuracy_score(y_test_b, y_pred)\n",
        "ketqua_ebd.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_b, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_b, y_pred))"
      ],
      "metadata": {
        "id": "ipAGIG9V3SP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>knn</h2>"
      ],
      "metadata": {
        "id": "cXkIPBybiJQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model_ebd = joblib.load('/content/drive/MyDrive/SIC/knn_model_ebd.pkl')"
      ],
      "metadata": {
        "id": "txJyeRYQFf7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_model_ebd.predict(X_test_b)\n",
        "accuracy = accuracy_score(y_test_b, y_pred)\n",
        "ketqua_ebd.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_b, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_b, y_pred))"
      ],
      "metadata": {
        "id": "nq4ylvs4AIm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vẽ kết quả"
      ],
      "metadata": {
        "id": "fUsXzrxpyDos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ketqua_tfidf, ketqua_ebd\n",
        "#SVC, KNN\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(x='SVC', ketqua_tfidf[0], width=0.3, label='SVC')\n",
        "plt.bar(x='KNN', ketqua_tfidf[1], width=0.3, label='KNN')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['SVC','KNN'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "drFE4wzolwuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}